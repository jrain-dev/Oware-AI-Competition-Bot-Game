Oware — Simple Oware Simulation and Q-Learning Agent

Overview

This repository contains a minimal Python implementation of the Oware (Mancala family) board game, a random agent, and a Q-Learning agent. It includes a simple simulation runner and a data logger that writes per-episode results to CSV.

Purpose

- Provide a small experimental environment for testing a Q-Learning agent on Oware.
- Serve as a starting point for training, hyperparameter tuning, and debugging simple reinforcement-learning agents.

Files

- `owareEngine.py` — Game engine and `OwareBoard` class. Implements board state, valid-move detection, move application (sowing and captures), scoring, and game-over checks.
- `agents.py` — Agent implementations:
  - `Agent` (base class)
  - `RandomAgent` (baseline agent)
  - `QLearningAgent` (simple tabular Q-learning with a defaultdict-backed Q-table)
- `dataLogger.py` — `DataLogger` writes a CSV file with per-episode summaries. Creates header on initialization and appends rows during training.
- `simulationRunner.py` — Top-level runner. Instantiates the board, agents, and logger, then runs episodes. By default it runs 20,000 episodes when executed as a script; use the `run_simulation` function for programmatic control.
- `training_log.csv` — Example output generated by a short run (committed here as an example of the output format).
- `requirements.txt` — Lists minimal Python dependency (`numpy`).

Quickstart

1. Create a Python 3 virtual environment (recommended):

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

2. Run a short simulation (5 episodes) from the terminal:

```bash
python3 -c "from simulationRunner import run_simulation; run_simulation(episodes=5)"
```

3. Or run the script directly (this runs the default configured episodes):

```bash
python3 simulationRunner.py
```

Output / Log Format

`training_log.csv` columns (header created by `DataLogger`):

- `Episode` — 1-based episode number.
- `Winner` — `Player0_QLearning`, `Player1_Random`, or `Draw`.
- `Final_Score_P0` — final score for player 0 (QL-learning agent in the runner).
- `Final_Score_P1` — final score for player 1 (random agent in the runner).
- `Exploration_Rate` — the Q-Learning agent's epsilon value at the end of the episode.

Design notes

- The Q-Learning agent uses a tabular Q-table indexed by the board state tuple and a fixed 12-element numpy array for actions.
- `agents.py` is intentionally small and simple — it’s easy to swap in a neural agent or richer state representation later.

Next steps / Ideas

- Add command-line arguments to `simulationRunner.py` (episodes, log filename, agent choices).
- Add basic unit tests (pytest) for `OwareBoard` methods: valid moves, move application, capture rules, end-of-game scoring.
- Replace tabular Q-Learning with a neural network (e.g., using PyTorch) for generalization.
- Add checkpointing for longer runs and resume capability.

License

This project is provided as-is for experimentation. Add a license file if you plan to open-source it publicly.
